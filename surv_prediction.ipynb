{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "from sksurv.metrics import concordance_index_censored"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pred_nodes = 1000\n",
    "hidden = [400,500,600,800]\n",
    "learning_rate = 1e-5\n",
    "epoch = 100"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x train (156, 246), positive ratio 0.058\n",
      "x valid (40, 246), positive ratio 0.050\n",
      "x test (50, 246), positive ratio 0.060\n"
     ]
    }
   ],
   "source": [
    "data_path = \"./data/hvgs\"\n",
    "f_list = os.listdir(data_path)\n",
    "\n",
    "file = os.path.join(data_path, f_list[0])\n",
    "\n",
    "df = pd.read_csv(file)\n",
    "# device = torch.device('cuda:0')\n",
    "device = torch.device('cpu')\n",
    "\n",
    "x = df.values[:, 3:]\n",
    "y = df.values[:, 1:3]\n",
    "seed = 42 ## shuffle random seed num\n",
    "duration_max = df.duration.max()\n",
    "\n",
    "## split time point into num_pred_nodes\n",
    "duration_reference = [\n",
    "    duration_max / num_pred_nodes * i for i in range(num_pred_nodes)\n",
    "]\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    x, y, random_state=42, train_size=0.8,stratify=y[:,1]\n",
    ")\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(\n",
    "    x_train, y_train, random_state=42, train_size=0.8,stratify=y_train[:,1]\n",
    ")\n",
    "\n",
    "\n",
    "print(f\"x train {x_train.shape}, positive ratio {y_train[:,1].sum()/len(y_train):.3f}\")\n",
    "print(f\"x valid {x_valid.shape}, positive ratio {y_valid[:,1].sum()/len(y_valid):.3f}\")\n",
    "print(f\"x test {x_test.shape}, positive ratio {y_test[:,1].sum()/len(y_test):.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_modifier(y):\n",
    "    \"\"\" \n",
    "    Converting duration and event label into vector type label.\n",
    "\n",
    "    eg1. Uncensored case\n",
    "        [3, 1] (duration, event) \n",
    "        duration_reference [0,2,4,6,8,10]\n",
    "        --> y_extend_lab: [0,0,1,1,1,1]\n",
    "        --> y_mask: [1,1,1,1,1,1]\n",
    "    eg2. Censored case\n",
    "        [2, 0] (duration, event)\n",
    "        --> y_extend_lab: [0,1,1,1,1,1]\n",
    "        --> y_mask: [1,0,0,0,0,0]\n",
    "\n",
    "    Args:\n",
    "        y (list, np.array): Label information including duration and event occurrence\n",
    "\n",
    "    Returns:\n",
    "        (y, y_extend_lab, y_mask)\n",
    "    \"\"\"\n",
    "    y_extend_lab = []\n",
    "    y_mask = []\n",
    "    for _y in y:\n",
    "        y_extend_lab.append((np.array(duration_reference) >= _y[0]) * 1)\n",
    "        if _y[1] == 0:\n",
    "            y_mask.append(~(np.array(duration_reference) >= _y[0]) * 1)\n",
    "        else:\n",
    "            y_mask.append(np.ones_like(y_extend_lab[-1]))\n",
    "    y_extend_lab = torch.tensor(np.array(y_extend_lab).astype(float))\n",
    "    y_mask = torch.tensor(np.array(y_mask).astype(int))\n",
    "    return y, y_extend_lab, y_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_mask = label_modifier(y_train)\n",
    "y_valid_mask = label_modifier(y_valid)\n",
    "y_test_mask = label_modifier(y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loader setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "class data_process(Dataset):\n",
    "    \"\"\" Return batch data including x, y, y_mask, y_orig\n",
    "    \"\"\"\n",
    "    def __init__(self, x, y, device=\"cpu\", x_mean=None, x_std=None) -> None:\n",
    "        super().__init__()\n",
    "        self.x = torch.tensor(x.astype(np.float32))\n",
    "        self.y_orig = torch.tensor(y[0].astype(int)).to(device)\n",
    "        self.y = y[1].to(device)\n",
    "        self.y_mask = y[2].to(device)\n",
    "        \n",
    "        if x_mean is None:\n",
    "            self.mean = self.x.mean(0)\n",
    "            self.std = self.x.std(0)\n",
    "            # self.x = (self.x - self.mean) / self.std\n",
    "        else:\n",
    "            self.mean = x_mean\n",
    "            self.std = x_std\n",
    "        self.x = (self.x - self.mean) / (self.std+1e-6)\n",
    "        self.x = self.x.to(device)\n",
    "    def return_mean_std(self):\n",
    "        return self.mean, self.std\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx], self.y_mask[idx], self.y_orig[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tr = data_process(x_train, y_train_mask)\n",
    "mean, std = data_tr.return_mean_std()\n",
    "data_val = data_process(x_valid, y_valid_mask, x_mean=mean, x_std=std)\n",
    "data_test = data_process(x_test, y_test_mask, x_mean=mean, x_std=std)\n",
    "dl_train = DataLoader(data_tr, batch_size=64, shuffle=True)\n",
    "dl_valid = DataLoader(data_val, batch_size=64)\n",
    "dl_test = DataLoader(data_test, batch_size=64)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "class model(nn.Module):\n",
    "    def __init__(self, inp_size, num_pred_tgt, hidden=[400,500,600,800]):\n",
    "        super().__init__()\n",
    "        hidden = [inp_size] + hidden + [num_pred_tgt]\n",
    "        self.linears = nn.ModuleList(\n",
    "            [nn.Linear(hidden[i], hidden[i + 1]) for i in range(len(hidden) - 1)]\n",
    "        )\n",
    "        self.n_linears = len(self.linears)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i in range(self.n_linears - 1):\n",
    "            x = self.linears[i](x)\n",
    "            x = F.relu(x)\n",
    "        x = self.linears[-1](x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.BCEWithLogitsLoss(reduction=\"none\")\n",
    "m = model(x.shape[1], num_pred_nodes, hidden).to(device)\n",
    "optimizer = torch.optim.Adam(m.parameters(), lr=learning_rate)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance check metric calc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def c_index_calc(logit, true_y):\n",
    "    pred = []\n",
    "    for i in range(logit.shape[0]):\n",
    "        print(i, logit[i])\n",
    "        min_idx = torch.where(logit[i]>0)[0].min().item()\n",
    "        pred.append(1/(min_idx+torch.sigmoid(logit[i][min_idx]).item()))\n",
    "    # print(pred)\n",
    "    event_indicator = (true_y[:, 1]==1)\n",
    "    event_time = true_y[:,0]\n",
    "    return concordance_index_censored(event_indicator,event_time, pred)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(model, data_loader, device):\n",
    "    loss_tot = 0\n",
    "    for i, batch in enumerate(data_loader):\n",
    "        model.eval()\n",
    "        logit = model(batch[0].to(device))\n",
    "        if i==0:\n",
    "            logit_tot = logit\n",
    "            y_tot = batch[3]\n",
    "        else:\n",
    "            logit_tot = torch.concat((logit_tot, logit), 0)\n",
    "            y_tot = torch.concat((y_tot, batch[3]), 0)\n",
    "        loss = (loss_fn(logit,batch[1].to(device)) * batch[2].to(device)).mean()\n",
    "        loss_tot+=loss\n",
    "    # print(logit_tot, y_tot)\n",
    "    c_idx = c_index_calc(logit_tot, y_tot)\n",
    "    return loss_tot/(i+1), c_idx"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
